== Resiliency

In this section we'll discuss resiliency in the world of microservices.
We'll have a look at what solutions Istio offers us already, transparent to the application.
Furthermore, we could implement resiliency in our application as well.

In order to not get confused by different behavior of coffee-shop `v1` and `v2`, let's first revert the changes of the last section, and only have a single subset `v1`.
This means, we change the coffee-shop virtual service, destination rule to the original one (`kubectl apply`), and remove the deployment `coffee-shop-v2` (`kubectl delete deployment coffee-shop-v2`).
After the deployment has been deleted, the pods with version 2 will also be terminated.

NOTE: Although we can always change infrastructure-as-code files and apply them via `kubectl apply`, deletions of resources in files won't be detected by Kubernetes, since it only re-applies and updates the currently provided resources.
Thus we need to explicitly `kubectl delete` obsolete resources.


=== Istio

Istio ships with some resiliency features, such as timeouts, circuit breakers, or retries.
Since the sidecar proxy containers intercept all connection from and to the application containers, they have full control over HTTP traffic.

That means, if our applications do not include for example connection timeouts, Istio can, as a last resort, ensure that a timeout will be triggered on a slow connection.
This enables us to add at least certain resiliency patterns without changing applications.

=== Timeouts

Istio enables us to add simple timeout definitions on rules.
For this to work, we need to ensure that the traffic is actually routed through virtual service rules, and not just uses the default rules, that is, the instances' labels need to match the specific subsets.

We'll enhance our coffee-shop virtual service rules with a timeout of 1 second:

[source,yaml]
----
cd ~/cloud-native-workshop*/coffee-shop/deployment/

cat <<EOF >coffee-shop-virtualservice.yaml
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: coffee-shop
spec:
  hosts:
  - "*"
  gateways:
  - coffee-shop-gateway
  http:
  - route:
    - destination:
        host: coffee-shop
        port:
          number: 9080
        subset: v2
      weight: 30
    - destination:
        host: coffee-shop
        port:
          number: 9080
        subset: v1
      weight: 70
    timeout: 1s
---
EOF
----

----
cd ~/cloud-native-workshop*/
kubectl apply -f coffee-shop/deployment/
----

NOTE: The `timeout` definition is contained in a specific route YAML object and will only be taken into account on that very rule.

We update the virtual service to our cluster.
Now, the overall time of the coffee order requests will not exceed one second.
If the coffee-shop application, or any backend with which we communicate synchronously takes longer than that the sidecar proxy will respond with an HTTP error instead.

==== Testing resiliency

The challenge, however, is now to see whether our changes took effect as desired.
We'd expect our applications to respond in much less than one second, therefore we would not see that error situation until it's in production.
Luckily, Istio ships with functionality that purposely produces error situations, in order to test the resiliency of our services.

The sidecars have two main means to do that: adding artificial delays, and failing requests.
We can instruct the routing rules to add these fault scenarios, if required only on a given percentage of the requests.

We modify the barista virtual service to add a 3 seconds delay for 50% of the requests:

[source, yaml]
----
cd ~/cloud-native-workshop*/barista/deployment/

cat <<EOF >barista-service.yaml
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: barista
spec:
  hosts:
  - barista
  http:
  - route:
    - destination:
        host: barista
        subset: v1
    fault:
      delay:
        fixedDelay: 3s
        percent: 50
---
EOF
----

If we apply this updated resource to the cluster, we will notice that some of the connections will in fact fail, after roughly 1 second.
We don't see any request taking the whole 3 seconds, due to the timeout on the coffee-shop routing rules:

----
cd ~/cloud-native-workshop*/barista/deployment/

kubectl apply -f .
----

----
while true; do
curl $URL/coffee-shop/resources/orders -i -XPOST \
  -H 'Content-Type: application/json' \
  -d '{"type":"Espresso"}' \
  | grep HTTP
sleep 1
done
----

NOTE: The `fault` property is only meant for testing purposes. Please don't apply this to any other environment where you don't want connections to be slowed down or to randomly fail.

Besides the obvious responses, we can also use our observability tools to inspect what is happening.
Have a look at the Grafana dashboards and the Jaeger traces again, to see how the failing requests are made visible.

This lab only covers timeouts and basic faults.
Istio also offers functionality for retries and circuit breakers which are also applied and configured declaratively via Istio resources.
Have a look at the further resources to learn more.


=== Application level

Building a resilient microservice is key when designing microservices.
Apart from the infrastructure resilience, sometimes more fine-grained application level resilience is required.

https://github.com/eclipse/microprofile-fault-tolerance/[MicroProfile Fault Tolerance^] provides a simple yet flexible solution to build a resilient microservice at the application-level.
It offers capabilities for timeouts, retries, bulkheads, circuit breakers and fallback.

In general, application-level resiliency is more fine-grained while Istio's behavior is more coarse-grained.
As a recommendation, we can use MicroProfile together with Istio's fault handling.

Looks like we've finished the last section! link:08-conclusion.adoc[Conclusion^].

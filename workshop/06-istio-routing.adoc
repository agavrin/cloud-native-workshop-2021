== Traffic management

In a previous part, we've examined how the basic building blocks of the Istio Networking API, virtual services, destination rules, and gateways work.
In this section, we'll have a look at some more advanced traffic routing performed by Istio, rather than by standard Kubernetes means.


=== Updated coffee-shop

In order to make some changes visible, we introduce a second version of our coffee-shop.
So far we've used the base version (`version: v1`) only.
For the following we add a second version of our microservice example which behaves slightly differently.

However, this version is not simply a new version of the same application, which could be updated and deployed in a zero-downtime manner (i.e. transition `v1` -> `v2`).
Rather, we specifically want two distinct versions to be available for a longer period of time, for example to perform A/B testing or canary releases.

Therefore, we make a change in the coffee-shop application to create a new version.
The orders resource currently returns the coffee type in all-caps, since that is the name of the `CoffeeType` enum.
We now change the Java code in the `OrdersResource` class to output the coffee type in lower case.

----
private JsonObject buildOrder(CoffeeOrder order) {
        return Json.createObjectBuilder()
                .add("type", order.getType().name().toLowerCase())
                .add("status", order.getStatus().name())
                .add("_self", buildUri(order).toString())
                .build();
    }
----

----
cd ~/cloud-native-workshop*/coffee-shop/
export NAME=<your name>
mvn package
docker build -t de.icr.io/cee-$NAME-workshop/coffee-shop:2 .
docker push de.icr.io/cee-$NAME-workshop/coffee-shop:2
----

Once we've re-built the Maven project, we build the coffee-shop Docker image, now with name `coffee-shop:2`.

We create a new deployment definition that will deploy our new coffee-shop application, with and leave the previous one untouched:

[source,yaml]
----
cd ~/cloud-native-workshop*/coffee-shop/deployment/

cat <<EOF >coffee-shop-v2.yaml
kind: Deployment
apiVersion: apps/v1
metadata:
  name: coffee-shop-v2
spec:
  selector:
    matchLabels:
      app: coffee-shop
      version: v2
  replicas: 1
  template:
    metadata:
      labels:
        app: coffee-shop
        version: v2
    spec:
      containers:
      - name: coffee-shop
        image: de.icr.io/cee-$NAME-workshop/coffee-shop:2
        imagePullPolicy: Always
        ports:
        - containerPort: 9080
        livenessProbe:
          exec:
            command: ["sh", "-c", "curl -f http://localhost:9080/"]
          initialDelaySeconds: 20
        readinessProbe:
          exec:
            command: ["sh", "-c", "curl -s http://localhost:9080/health | grep -q coffee-shop"]
          initialDelaySeconds: 40
EOF
----

Pods managed by this deployment will receive the labels `app: coffee-shop` and `version: v2`.
The coffee-shop service would simply forward requests to this pod as well, however, since our traffic is routed by the Istio proxies, only the previous instances will receive traffic for now.

Apply all changes with
----
cd ~/cloud-native-workshop*/
kubectl apply -f coffee-shop/deployment/
----


We ensure that our new deployment and pods have been created:

----
kubectl get deployments
kubectl get pods
----


=== A/B testing with Istio

A/B testing is a method of running tests against two separate service versions in order to determine which performs better according to a set of defined metrics.
To add a second version of our coffee-shop service we enhance the coffee-shop destination rule and add a second subset:

[source,yaml]
----
cd ~/cloud-native-workshop*/coffee-shop/deployment/

cat <<EOF >coffee-shop-destinationrule.yaml
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: coffee-shop
spec:
  host: coffee-shop
  subsets:
  - name: v1
    labels:
      version: v1
  - name: v2
    labels:
      version: v2
EOF
----

We apply the updated destination rule to the cluster.
----
cd ~/cloud-native-workshop*/
kubectl apply -f coffee-shop/deployment/
----

As you will notice, this will have no effect on the current scenario.
If you have a look at the current coffee-shop virtual service, you'll notice that we route all incoming traffic to `v1` subset:

----
kubectl get virtualservice coffee-shop --output yaml
----

We enhance the routing rule to route to subset `v2` as well, but only for traffic which matches the HTTP header of a Firefox user agent:

[source,yaml]
----
cd ~/cloud-native-workshop*/coffee-shop/deployment/

cat <<EOF >coffee-shop-virtualservice.yaml
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: coffee-shop
spec:
  hosts:
  - "*"
  gateways:
  - coffee-shop-gateway
  http:
  - match:
    - headers:
        user-agent:
          regex: '.*Firefox.*'
    route:
    - destination:
        host: coffee-shop
        port:
          number: 9080
        subset: v2
  - route:
    - destination:
        host: coffee-shop
        port:
          number: 9080
        subset: v1
---
EOF
----

----
cd ~/cloud-native-workshop*/
kubectl apply -f coffee-shop/deployment/
----

The newly introduce rule will route the traffic from Firefox browsers to all instances that are contained in the `v2` service subset and leave the rest untouched, that is, everything else still routes to the `v1` subset.
In Istio `VirtualService` rules, there can be only one rule for each service and therefore when defining multiple https://istio.io/docs/reference/config/istio.networking.v1alpha3/#HTTPRoute[HTTPRoute^] blocks, the order in which they are defined in the YAML file matters.

If we apply these changes to the cluster, we can now see a different behavior for requests that originate from a Firefox browser.
Similarly, we can simulate that behavior from the command line, if we pass a corresponding header to `curl`:

----
curl $URL/coffee-shop/resources/orders -i -XPOST \
  -H 'User-agent: Mozilla/5.0 (X11; Linux x86_64; rv:62.0) Gecko/20100101 Firefox/62.0' \
  -H 'Content-Type: application/json' \
  -d '{"type":"Espresso"}'

curl $URL/coffee-shop/resources/orders \
  -H 'User-agent: Mozilla/5.0 (X11; Linux x86_64; rv:62.0) Gecko/20100101 Firefox/62.0'
----


=== Canary Deployments

In canary deployments, newer versions of services are incrementally rolled out to users to minimize the risk and impact of any bugs introduced by the newer version.
To begin incrementally routing traffic to the newer version of the coffee-shop service, we modify its virtual service:

[source,yaml]
----
cd ~/cloud-native-workshop*/coffee-shop/deployment/

cat <<EOF >coffee-shop-virtualservice.yaml
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: coffee-shop
spec:
  hosts:
  - "*"
  gateways:
  - coffee-shop-gateway
  http:
  - route:
    - destination:
        host: coffee-shop
        port:
          number: 9080
        subset: v2
      weight: 30
    - destination:
        host: coffee-shop
        port:
          number: 9080
        subset: v1
      weight: 70
---
EOF
----


----
cd ~/cloud-native-workshop*/
kubectl apply -f coffee-shop/deployment/
----


In this modified rule, the routed traffic is split between the two subsets of the coffee-shop service (70% to `v1` and 30% to `v2`).
Traffic to the modernized version of our service is controlled on a percentage basis to limit the impact of any unforeseen bugs.
This rule can be modified over time until eventually all traffic is directed to the newer version of the service.
This would be part of an automated process, typically realized by a Continuous Deployment pipeline.

We can see this rule in action by accessing our application again.
If you're accessing the example through a browser, make sure that you're performing a hard refresh to remove any browser IP address caching.
You should notice that the coffee-shop should swap between the first and second version at roughly the weight you specified.

Now that we've managed some traffic with Istio, let's have a closer look how to make our microservices more resilient in the link:07-resiliency.adoc[next section].

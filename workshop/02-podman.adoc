== podman

In order to run our applications in podman containers, we need podman images first which are built from ``podmanfile``s.
Therefore, we create two `podmanfile` files, one under each project directory.


=== podman images

In order to fetch the required dependencies, we need to execute the following scripts once:

----
cd ~/cloud-native-workshop*/
coffee-shop/liberty/prepare.sh
barista/liberty/prepare.sh
----

Let's start with the coffee-shop application.
Under `coffee-shop/`, we create a `podmanfile` with the following contents:

[source,podmanfile]
----
cd ~/cloud-native-workshop*/coffee-shop

cat <<EOF >podmanfile
FROM podman.io/library/open-liberty:21.0.0.4-full-java8-openj9

COPY liberty/extension /liberty/usr/extension/
COPY liberty/server.xml /config/

COPY target/coffee-shop.war /config/dropins/
EOF
----

The image declaration after `FROM` specifies the base image for our application.
We use that as a starting ground.
For now, we can assume that this base image already includes an application server, here Open Liberty, and a Java runtime.

`/config/` and `/config/dropins/` are directories inside the container, where we can place our configuration and deployment artifacts.

The `COPY` command copies resources from the host to the container, that is, they will end up in the resulting image.
The `liberty/server.xml` file contains the application-specific server configuration including the server features that we want to use in our service.
Since we deploy only a single applications to each running server and thus running container, the server configuration is exclusively used for the individual applications.

Take a moment to have a brief look at the `liberty/server.xml` file.
We'll come back to it later.
Now, we create a similar `podmanfile` for the barista application, under `barista/`:

[source,podmanfile]
----
cd ~/cloud-native-workshop*/barista

cat <<EOF >podmanfile
FROM podman.io/library/open-liberty:21.0.0.4-full-java8-openj9

COPY liberty/extension /liberty/usr/extension/
COPY liberty/server.xml /config/

COPY target/barista.war /config/dropins/
EOF
----


=== Building

In order to build the images, we issue the following podman command: `podman build -t <image>:<tag> .` (including the dot at the end).

Let's setup a variable to have your name, to be used as unique identifier, also provide unique port number:
----
export NAME=<yourname>
export PORT=9080
----

For the coffee-shop application this looks as follows:

----
cd ~/cloud-native-workshop*/
cd coffee-shop/
podman build -t de.icr.io/cee-$NAME-workshop/coffee-shop:1 .
----

The image name, here `de.icr.io/cee-<your-name>-workshop/coffee-shop:1`, implicitly contains the location of the image registry, image name, and version (or tag).
If an image with that name is being pulled or pushed, downloaded or uploaded, respectively, podman thus already knows the destination.

The `de.icr.io/...` part indicates the used cloud service and our user name and namespace.

Again, we repeat this step for the barista application:

----
cd ~/cloud-native-workshop*/
cd barista/
podman build -t de.icr.io/cee-$NAME-workshop/barista:1 .
----

Congratulations, now we have two images stored locally, for the coffee-shop and barista applications that we can now run as podman containers!


=== Running

If we want, we can already run the created podman images locally on our machines.

In order do to that, we have to keep in mind, that the coffee-shop application will connect to the barista backend, thus needs to resolve and access the second running container.
If we have a look at the source code of the coffee-shop application, specifically the `Barista` gateway, we notice that the application will simply connect to `http://barista:9080` -- host name `barista`.

This will be possible since we specify that our microservices will run in a cloud-native environment, where we can control the environment independent of the applications.
Thus we make sure that our applications will be reachable under logical names, such as `coffee-shop` or `barista`.

For locally running container, we can assign names to the running containers that are resolvable within other containers that run inside the same local Podman network.
This requires to specify both the container names and the network at runtime.

Check if you have any containers running:

----
podman ps
----

If you have any containers running,

We start our containers as follows:

----
podman run --rm -d \
  --name barista \
  --network podmannet \
  de.icr.io/cee-$NAME-workshop/barista:1

podman run --rm -d \
  --name coffee-shop \
  --network podmannet \
  -p $PORT:9080 \
  de.icr.io/cee-$NAME-workshop/coffee-shop:1
----

These commands run our two applications as Podman containers.
The `--rm` flag will remove the containers after they have been stopped, `-d` runs the containers in the background, `--name` specifies the logical container names, `--network` makes sure both containers will run in the correct network, and the `-p` declarations forward the local containers ports to the localhost.

To start from scratch, you can kill all running podman containers with

----
podman kill $(podman ps -q)
----

The two running containers can now resolve and access each other by their logical container names, which allows our coffee-shop application to connect to the barista backend using the `barista` host name.
The reason why we're doing this, is because Kubernetes will support a very similar resolution of logical names.


==== Accessing our applications

Now we can access and test our running microservices for the first time!
Since they communicate using HTTP we can use any REST client of our choice, for example `curl`.


==== Health check resources

We could simply ask the application for the coffee orders in the system but for our cloud-native environment we might want a more basic way to check whether our application is up and running.
Therefore, we create health check resources.
We could do so by using plain REST resources, for example implemented by JAX-RS.
What's also possible, and done with minimal effort, is to use MicroProfile Health to create HTTP health checks.

Therefore, we create a class with the name `HealthResource` or similar, in both the barista and coffee shop projects, for example under the `*.boundary` package:

[source,java]
----
import org.eclipse.microprofile.health.*;
import javax.enterprise.context.ApplicationScoped;

@Readiness
@ApplicationScoped
public class HealthResource implements HealthCheck {

    @Override
    public HealthCheckResponse call() {
        return HealthCheckResponse.named("barista").withData("barista", "ok").up().build();
    }
}
----

This shows the example for the barista application.
For our coffee shop example please adjust the strings accordingly.

That's it!
However, if we want these changes to be included in our running containers, we need to re-build the Java applications and the Podman images again, of course.

After the applications have been started, we should be able to access the coffee shop via the local port `9080` and the default MicroProfile health resource:

----
curl localhost:$PORT/health -i
----

This accesses the health check resource and will hopefully give you a successful HTTP response.
The `-i` flag causes the HTTP response headers to be printed.


==== Ordering coffee

Now, we can finally ask for the coffee orders:

----
curl localhost:$PORT/coffee-shop/resources/orders
----

This will give us the coffee orders that are in the system returned as JSON.
No orders have been created, thus the array is empty.

Let's change this and create a coffee order!

If we have a look at the JAX-RS resource in the coffee-shop application, we can see that to create a new coffee order, we have to POST a JSON object containing the coffee _type_.
Using `curl` this looks as follows:

----
curl localhost:$PORT/coffee-shop/resources/orders -i -XPOST \
  -H 'Content-Type: application/json' \
  -d '{"type":"Espresso"}'
----

`-XPOST` specifies the `POST` HTTP method, `-H` the HTTP header, so the service knows that we're sending the JSON content type, and `-d` specifies the data that we send as HTTP request body.
Sending this command hopefully yields us a successful `201 Created` response, the information that our coffee order is in the system.

We can double-check this by querying the resource for all coffee orders again, similar to before, which now should respond with a JSON array that contains our order.

If that's the case, congratulations!
You've just built, run, and manually tested cloud-native microservices running in Podman containers.


=== Pushing

In order to make our Podman images not just locally accessible, we will push them to a container registry in the cloud.
Then we can later pull them from any environment, like a managed Kubernetes cluster.

We push our Podman images with the following commands:

----
podman push de.icr.io/cee-$NAME-workshop/coffee-shop:1
podman push de.icr.io/cee-$NAME-workshop/barista:1
----

You will notice, that the second `push` commands runs much faster and outputs that almost all layers already exist in the remote repository.
This thanks to the copy-on-write file system which podman uses internally and save us developers an enormous amount of time and bandwidth.
The same is true for re-building images.
podman recognizes which commands of the podman build need to be re-executed, and only performs these and the following.

This is the reason why especially for cloud-native applications it makes sense to craft thin deployment artifacts.
The WAR files that comprise our applications only contain the business logic that is part of our application, no implementation details.
The base image, i.e. the application server or its configuration doesn't change that frequently, therefore we're mostly shipping our (small) application only.

Now, that our microservices are running as podman containers already, let's see how we bring Kubernetes into the game in the link:03-kubernetes.adoc[next section].

== Istio

Istio is a service mesh technology which enables us to add cross-cutting concerns to our microservices without changing our implementations.
Istio add sidecar containers to each running pod, that act as proxies to our applications.
Every running pod will get a distinct proxy sidecar container which intercepts, inspects or enhances the connection.

Istio adds default routes in between the Kubernetes services.
However, it's a best practice to explicitly add Istio routes between the desired services.

The way how Istio integrates with Kubernetes is that it enhances Kubernetes' API model to add Istio resource types.
This enables developers to use Istio resources with the same experience as if they were included with Kubernetes, including the `kubectl` CLI.

We will see that the following Istio resources are defined in the YAML format as well and look very much like Kubernetes resources.
We place the following files in the same directories as the Kubernetes definitions, either in single files or one per resource.


=== Virtual Service

We create an Istio virtual service for the *barista* application:

[source,yaml]
----
cat <<EOF >barista-routing.yaml
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: barista
spec:
  hosts:
  - barista
  http:
  - route:
    - destination:
        host: barista
        subset: v1
---
EOF
----

A virtual service defines the routing rules for a service that is part of the mesh.
By default, all Kubernetes services are part of the service mesh.

The routing to the barista service specifies that all requests that access the barista service are routed to the barista instances with the subset (i.e. additional label) `v1`.
In practice this acts as a default route.
The subsets, among other request policies are defined in the destination rules of the corresponding services.


=== Destination Rule

We create a destination rule for the barista application which defines the subset `v1`.

[source,yaml]
----
cat <<EOF >>barista-routing.yaml
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: barista
spec:
  host: barista
  subsets:
  - name: v1
    labels:
      version: v1
---
EOF
----

The subset defines that pods that contain the desired label, here `version` are considered to be part of the subset.
Including a `version` label is another best practice.

We can put these two Istio resources into a single file, for example named `routing.yaml` or into separate ones.

In order to make this example work, our pods have to include the label.
Therefore, we have to change and re-apply both deployment definitions.

We will add the `version: v1` label to the labels section of the pod template metadata, again, to both our application deployments (the YAML files).
Then, we update the deployment definitions against our cluster again (`kubectl apply`).


=== Gateway

In order to make our application accessible from outside the service mesh, we define a gateway resource.
Instead of Kubernetes ingress resources which we defined before, Istio gateways enable ingress traffic to be already inspected and routed by Istio.

We create an Istio routing definition with the following contents to define a gateway that routes traffic into the cluster:

[source,yaml]
----
cat <<EOF >coffee-shop-gateway.yaml
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: coffee-shop-gateway
spec:
  selector:
    istio: ingressgateway
  servers:
  - port:
      number: 80
      name: http
      protocol: HTTP
    hosts:
    - "*"
---
EOF
----

The gateway specifies the wildcard host (`*`) which matches all host names or IP addresses.
It needs to be bound to a virtual service via the `gateways` property.
This will be part of the coffee-shop's virtual service specification:

[source,yaml]
----
cat <<EOF >coffee-shop-virtualservice.yaml
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: coffee-shop
spec:
  hosts:
  - "*"
  gateways:
  - coffee-shop-gateway
  http:
  - route:
    - destination:
        host: coffee-shop
        port:
          number: 9080
        subset: v1
---
EOF
----

That routing now looks slightly different.
Since we define a specific gateway, only traffic from that gateway will be routed to that virtual service, not from any other service inside the mesh (this would require us to explicitly add the default `mesh` gateway as well).

The HTTP routing rule also requires us the specify the port number, since the incoming traffic originated from a different port number (`80`).

The following contents define the resource for the coffe-shop destination rule, which looks similar to the barista's.

[source,yaml]
----
cat <<EOF >coffee-shop-destinationrule.yaml
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: coffee-shop
spec:
  host: coffee-shop
  subsets:
  - name: v1
    labels:
      version: v1
---
EOF
----

Apply all changes with
----
kubectl apply -f coffee-shop/deployment/
kubectl apply -f barista/deployment/
----

==== Accessing our applications

If we now want to run our application and access it through the service mesh, we will access the gateway from outside the cluster.
This requires, of course, that the gateway resource and all virtual service and destination rule resources have been applied to the mesh.

If we have created a free cluster we, again, need to access the (gateway) service through the cluster's node IP address.
Thus, we retrieve the node port of the `istio-ingressgateway` service, a service that's shipped by the Istio installation:

----
kubectl get services -n istio-system istio-ingressgateway
----

We can get the HTTP/2 node port directly by using the following Go template:

----
export PORT=$(kubectl get services -n istio-system istio-ingressgateway --template '{{range .spec.ports}}{{if eq .name "http2"}}{{.nodePort}}{{end}}{{end}}'); echo $PORT
----

[NOTE]
====================
As a reminder, as seen in the last section, we'll retrieve the node IP address with one of the following commands.

----
ibmcloud ks workers --cluster mycluster-free
----

If you have the `jq` CLI tool installed, you can also directly extract the IP address by invoking:

----
export IP=$(ibmcloud ks workers --cluster mycluster-free --json | jq -r '.[0].publicIP'); echo $IP
export URL="$IP:$PORT"; echo $URL
----
====================

We can then access the service using the node IP address and the node port of the `istio-ingressgateway` service:

----
curl $URL/health | jq
...
curl $URL/coffee-shop/resources/orders -i
----

This scenario works completely without the Kubernetes ingress resource.
Now, only the Istio resources would be required, besides the deployments and services.

We can similarly use the `/orders` resource to create new coffee orders:

----
curl $URL/coffee-shop/resources/orders -i -XPOST \
  -H 'Content-Type: application/json' \
  -d '{"type":"Espresso"}'
----

[NOTE]
====================
If we have a paid cluster, we can acquire the gateway IP address through the load balancer IP address of the `istio-ingressgateway` service:

----
kubectl get services -n istio-system istio-ingressgateway \
  -o jsonpath='{.status.loadBalancer.ingress[0].ip}'
----

We use this IP address and the default HTTP ports (`80` or `443`, respectively) to access the application from outside the cluster:

----
curl <gateway-ip-address>/health -i
----
====================

In theory, this means that both of our services are working as expected and can communicate with each other.
However, this assumption, or observation is hardly enough for a system that runs in production.

Let's see how Istio improves our observability in the link:05-istio-observability.adoc[next section].
